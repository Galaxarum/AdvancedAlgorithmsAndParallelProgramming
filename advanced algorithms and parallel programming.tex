\documentclass{article}
\usepackage{algorithm,algorithmic}
\usepackage{mathtools}

\usepackage{qtree}
\usepackage{hyperref, titlesec}

\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newcommand{\sectionbreak}{\clearpage}

\author{Matteo Secco}
\title{Advanced Algorithms and Parallel Programming}


\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\section{Algorithm recap}
\paragraph{Algorithm} any well defined computational procedure that takes some values as input, and produces some values as output. It must terinate in a finite number of steps
\paragraph{Algorithm analysis} Theoretical study of computer-program performance and resource usage
Performance is not the only thing that matters. For example
\begin{itemize}
\item modularity
\item maintainability
\item user-friendlyness
\end{itemize}
are some important aspects as well
\paragraph{Why do we study algorithms and performance?} 
\begin{itemize}
\item to better understand scalability
\item to understand what is feasible and what is not
\item to have a formal language to talk about a program behaviour
\end{itemize}
\begin{algorithm}
\caption{Insertion sort}
\begin{algorithmic}
\REQUIRE $A,n$
\REQUIRE $len(A)=n$
\ENSURE $A$ is sorted
\FOR{$j\leftarrow 2$ \textbf{to} $j \leq n$}
	\STATE $key \leftarrow A[j]$
	\STATE $i \leftarrow j-1$
	\WHILE{$i>0$ \and $A[i]>key$}
		\STATE $A[i+1] \leftarrow A[i]$
		\STATE $i \leftarrow i-1$
	\ENDWHILE
	\STATE $A[i+1]=key$
\ENDFOR
\end{algorithmic}
\end{algorithm}
\paragraph{Obvious observations}
\begin{itemize}
\item Running time depends on the input
\item Running time has to be parametrized on the length of the input
\item We generally look for upper limits because those are more interesting in real world
\end{itemize}
\paragraph{Kind of analysis}
\begin{description}
\item[Worst case] $T(n)=$maximum time of the algorithm on any input of size $n$
\item[Average case] $T(n)=$expected time over all inputs of size $n$. Requires assumption on statistical distribution of the inputs
\item[Best case] Easy to be cheated with slow algorithms that works very well on \underline{specific} inputs
\end{description}

\paragraph{Asympthotic Analysis} Ignore machine-dependent constraints and look at the behaviour of $T(n)$ as $n \rightarrow \infty$

\subsection{Asymptotic notations}
\paragraph{O-notation} provides upper bounds to execution times
\[
O(g(n)) = 
\{ 
f(n): 
\quad \exists c>0, n_0>0 :
0 \leq f(n) \leq c \cdot g(n) 
\quad \forall n \geq n_0 
\}
\]
\paragraph{$\Omega$-notation} provides lower bounds to execution times
\[
\Omega(g(n)) = 
\{
f(n):
\quad \exists c>0, n_0>0 :
0 \leq c \cdot g(n) \leq f(n)
\quad \forall n \geq n_0
\}
\]
\paragraph{$\Theta$-notation} provides tight bounds to execution times
\[\Theta(g(n))=O(g(n)) \cap \Omega(g(n))\]

\paragraph{Merge sort}
\begin{algorithm}
\caption{Merge-Sort}
\begin{algorithmic}
\REQUIRE $A, n$
\REQUIRE $len(A)=n$
\IF{$n=1$}
	\RETURN $A$
\ENDIF
\STATE $L \leftarrow \text{Merge-Sort}\left(A\left[1..\ceil*{\frac{n}{2}}\right]\right)$
\STATE $R \leftarrow \text{Merge-Sort}\left(A\left[\ceil*{\frac{n}{2}}+1..n\right]\right)$
\RETURN Merge$(L,R)$
\end{algorithmic}
\end{algorithm}

\section{Recurrence analysis}
\subsection{Recursion tree analysis} applied to merge sort
\[
T(n) = \begin{cases}
\Theta(1)					&	\text{if }n=1\\
2T\left(\frac{n}{2}\right)+\Theta(n)		&	\text{if }n>1
\end{cases}
\]
\paragraph{Example} $T(n)=2T\left(\frac{n}{2}\right)+cn$\\
\qtreecenterfalse
\Tree[.{$T(n)$} ]
$\rightarrow$
\Tree [.{$cn$} {$T\left(\frac{n}{2}\right)$} {$T\left(\frac{n}{2}\right)$}  ]
$\rightarrow$
\Tree [.{$cn$} 
		[.{$\frac{cn}{2}$} {$T\left(\frac{n}{4}\right)$} {$T\left(\frac{n}{4}\right)$} ]
		[.{$\frac{cn}{2}$} {$T\left(\frac{n}{4}\right)$} {$T\left(\frac{n}{4}\right)$} ]
	]
$\rightarrow$
\Tree [.{$cn$}
		[.{$\frac{cn}{2}$} {$\frac{cn}{4}$} {$\frac{cn}{4}$} ]
		[.{$\frac{cn}{2}$} {$\frac{cn}{4}$} 
			\qroof{$\Theta(1)\quad ...\quad\Theta(1)$}.{...}
		]
	]\\
\begin{table}[H]
\begin{tabular}{l|l|l}
	\textbf{Recursion conplexity}	&	\textbf{Base case complexity}&	\textbf{Total Complexity}\\
	\hline
	$h=\log(n)$ 						& 	\#leaves$=n$					&	\\
	each level adds up to $cn$		& 	$\Theta(1)$ per leave		&	\\
	\hline
	$h\cdot cn=cn\log (n)$			&	$n$							&	$O(n\log (n)$\\
\end{tabular}
\end{table}


\subsection{Analysis by substitution}
\begin{description}
\item[Guess] the form of the solution
\item[Verify] by induction
\item[Solve] for constraints
\end{description}

\paragraph{Example}
$T(n)=4T\left(\frac{n}{2}\right)+n$	(and $T(1)=\Theta(1)$)\\
\begin{itemize}
\item Guess $T(n)=O(n^3)$
\item Find some $k<n$ such that $T(k)\leq ck^3$
\item Prove $T(n)\leq cn^3$ by induction
\end{itemize}

\subsection{Master theorem}
Applies to recurrencies of the form $T(n)=aT\left(\frac{n}{b}\right)+f(n)$\\ where $a\geq 1, b>1, f>0$ for $n \rightarrow \infty$
\begin{align*}
f(n)		&=O	(n^{\log_b a - \epsilon})		&\rightarrow& 	T(n)= \Theta(n^{\log_b a})\\
f(n)		&=	\Theta(n^{\log_b a}\log^k n)		&\rightarrow& 	T(n)=\Theta(n^{log_b a} \log^{k+1} n)\\
f(n)		&=	\Omega(n^{\log_b a + \epsilon}) 	&\rightarrow& 	T(n)=\Theta(f(n))
\end{align*}

\section{Divide and Conquer}
\begin{description}
\item[Divide] the problem into subproblems
\item[Conquer] the subproblems recursively
\item[Combine] subproblem solutions
\end{description}

\paragraph{Merge sort}
\begin{description}
\item[Divide] split in half
\item[Conquer] Sort the 2 subarrays
\item[Combine] Linear-time merge
\end{description}

\paragraph{Binary search}
\begin{description}
\item[Divide] Check middle element
\item[Conquer] Search 1 subarray
\item[Combine] Return result up
\end{description}

\paragraph{Compute $a^n$}
\[a^n=\begin{cases}
	a^{n/2} \cdot a^{n/2}					&	\text{if n is even}\\
	a \cdot a^{(n-1)/2} \cdot a^{(n-1)/2}	&	\text{if n is odd}
\end{cases}
\]
\[T(n)=T\left(\frac{n}{2}\right)+\Theta(1)=\Theta(\log n)\]



\section{Parallel Random Access Machine}
\paragraph{RAM:} abstract device having
\begin{itemize}
\item Unbounded number of memory cells
\item Unbounded size for each memory cell
\item Instruction set including simple operations, data operations, comparations, branches
\item All operations take unitary time
\item Time complexity = \# instructions executed
\item Space complexity = \# memory cells used
\end{itemize}
\paragraph{PRAM:} abstract device for designing parallel algorithms. $M'$ is a system $<M,x,y,A>$ of infinitely many
\begin{itemize}
\item RAMs $M_i$ called processors. Each is assumed to be itentical to the others and recognize its own index
\item Input cells $X_i$
\item Output cells $Y_i$
\item Shared memory cells $A_i$
\end{itemize}

\paragraph{Computation step} consists of 5 phases. In parallel, each processor:
\begin{itemize}
\item Reads a value from one of $X_i$
\item Reads a value from one of $A_i$
\item Performs some internal computation
\item May write into one of the $Y_i$
\item May write into one of the $A_i$
\end{itemize}
Some peculiarities to highlight:
\begin{itemize}
\item Some processors may remain idle
\item More processor can safely read the same memory cell
\item When more processor write the same cell at the same time a \textbf{write conflict} occours
\end{itemize}

\subsection{Conflict Management} PRAM are classified based on how they manage conflicts. 
\begin{description}
\item[Exclusive Read (ER)] processors can read only from \underline{distinct} memory cells
\item[Concurrent Read (CR)] processors can simultaneously read from \underline{any} memory cell
\item[Exclusive Write (EW)] processors can symultaneously write to \underline{distinct} memory cells
\item[Concurrent Write (CR)] processors can simultaneously read from \underline{any} memory cell
\end{description}
Four possible combination may happen, but only three (EREW, CREW, CRCW) are interesting. This is because in  real applications, it is pointless to have read capabilities stricter than write ones.
\paragraph{Concurrent Writes} allows three models to determine what will be actually written in case of a conflict:
\begin{description}
\item[Priority CW] each processor is assigned a (unique) priority. The value from the processor with higher priority  is the one actually written
\item[Common CW] the write completes $\iff$ all the values to be written are equal
\item[Arbitrary/Random CW] one randomly chosen processor is allowed to complete the write
\end{description}

\subsection{Strenghts of PRAM}
\begin{description}
\item[Natural:] the number of operations per cycle having $p$ processors is at most $p$
\item[Strong:] any processor can read/write any shared cell in unit time
\item[Simple:]  abstracts from communication/synchronization overheads, making it easier to evaluate complexity and correctness
\item[Can be used as benchmark:] if a problem has no feasible solution in PRAM, it neither has on any parallel machine
\end{description}

\subsection{Computational power} $A$ is computationally stronger than $B$ $\iff$ any algorithm written for $B$  will run unchanged on $A$ in the same parallel time and with  the same basic properties.
\[
\substack{\textit{Most powerful}\\ \textit{Least realistic}}
\qquad
\text{Priority}\geq \text{Arbitrary}\geq \text{Common} \geq \text{CREW} \geq \text{EREW}
\qquad
\substack{\textit{Least powerful}\\ \textit{Most realistic}}
\]

\subsection{Definitions}
\begin{description}
\item[$T^*(n)$] Time to solve on \underline{one} processor, using the best sequential algorithm
\item[$T_p(n)$] Time to solve on $p$ processors
\item[$SU_p(n)=\frac{T^*(n)}{T_p(n)}$] Speedup on $p$ processors
\item[$E_p(n)=\frac{T_1(n)}{p\cdot T_p(n)}$] Efficiency (time on $1$ / time that could be used on $p$)
\item[$T_\infty(n)$] Shortest run time for any value of $p$
\item[$C(n)=P(n)\cdot T(n)$] Cost (in terms of time and processors
\item[$W(n)$] Work=total \#operations
\end{description}

\subsection{Amdahl's vs Gustafson's law}
\subsubsection{Amdahl's law}
\paragraph{Computation model} consists in interleaved segments. Segments can either be serial (no speedup from parallelization) or parallelizable (allowing for speedup)\\
\[T_p>\frac{T_1}{P} \implies SU>P\]
The length of the parallelizable part is a \textbf{fixed} fraction $f$, and the sequential length is $1-f$.
\[
SU(P,f)=\frac{T_1}{T_p}
=\frac{T_1}{\underbrace{T_1\cdot (1-f)}_\text{sequential part}+\underbrace{\frac{T_1\cdot f}{P}}_\text{parallel part}}
=\frac{1}{(1-f)+\frac{f}{P}}
\]
\[\lim_{P\to \infty} SP(P,f) = \frac{1}{1-f}
\]

\subsubsection{Gustafson's law}
\paragraph{Differences in the model}
\begin{itemize}
\item $f$ is not fixed
\item Absolute serial time is fixed
\item Parallel problem size is increased to exploit more processors
\item Serial time $s$ is fixed
\item Parallel time per processor $1-s$ is fixed
\end{itemize}
\[SU(P)=\frac{T_1}{T_p}
=\frac{s+\overbrace{P\cdot (1-s)}^\text{full parallel computation}}{s+\underbrace{(1-s)}_{\substack{\text{parallel computation}\\ \text{on each processor}}}}
=s+P\cdot (1-s)
\qquad \implies \textbf{Linear speedup}
\]

\section{Complexity classes}
\subsection{P complexity}
Complexity class P contains decision problems which can be solved by a deterministic Turing machine in polynomial time
\paragraph{P-complete} A problem is P-complete if it is in P and any problem in P can be reduced to it by an appropriate reduction

\subsection{NC complexity} Nick's Class is the set of problem decidable in polylogarithmic time on a parallel computer with a polynomial number of processors
\[
\exists c,k: \: \text{the problem can be solved in time }
O(log^c n)
\text{ using } O(n^k) \text{ processors}
\]
\end{document}